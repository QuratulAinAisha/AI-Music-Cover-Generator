{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCHiGNedakjg",
        "outputId": "3df55aa1-e1d8-470a-8131-ebf88d0d0bbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.8.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DFhNwmPaoIL",
        "outputId": "bb429c2a-26fa-433e-b386-67a3475909c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.2.19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install --upgrade \"numba>=0.56\" \"llvmlite>=0.39\"\n",
        "!pip install git+https://github.com/deezer/spleeter\n"
      ],
      "metadata": {
        "id": "_CuSOUKifH9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Upgrade pip and install dependencies -----\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install -q yt-dlp  ffmpeg-python torch librosa soundfile\n",
        "!apt-get -qq install ffmpeg\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "# ----- Helper Functions -----\n",
        "def download_youtube_audio(youtube_url, output_path=\"downloaded_song.mp3\"):\n",
        "    \"\"\"\n",
        "    Download the audio of a YouTube video using yt-dlp.\n",
        "    Due to post-processing, the final file may have a double extension.\n",
        "    This function checks for that and returns the correct file path.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Downloading YouTube audio using yt-dlp...\")\n",
        "    from yt_dlp import YoutubeDL\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': output_path,\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([youtube_url])\n",
        "    # Check for double extension: if output_path doesn't exist, try output_path+\".mp3\"\n",
        "    final_path = output_path\n",
        "    if not os.path.exists(final_path):\n",
        "        alt_path = output_path + \".mp3\"\n",
        "        if os.path.exists(alt_path):\n",
        "            final_path = alt_path\n",
        "    print(f\"[INFO] Download completed: {final_path}\")\n",
        "    return final_path\n",
        "\n",
        "def separate_vocals(audio_file, output_dir=\"separated\"):\n",
        "    \"\"\"\n",
        "    Use Spleeter to separate vocals and instrumental.\n",
        "    If the expected files (vocals.wav, accompaniment.wav) are not found,\n",
        "    try a fallback method based on the full basename.\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"[INFO] Separating vocals and instrumental with Spleeter...\")\n",
        "\n",
        "    # Use \"python3\" so that the correct interpreter (with spleeter installed) is used.\n",
        "    command = f\"python3 -m spleeter separate -p spleeter:2stems -o {output_dir} {audio_file}\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Spleeter stdout:\")\n",
        "        print(result.stdout)\n",
        "        print(\"Spleeter stderr:\")\n",
        "        print(result.stderr)\n",
        "        raise RuntimeError(\"Spleeter command failed with return code \" + str(result.returncode))\n",
        "\n",
        "    # Use the base name from os.path.splitext to determine the expected folder.\n",
        "    expected_folder = os.path.join(output_dir, os.path.splitext(os.path.basename(audio_file))[0])\n",
        "\n",
        "    if not os.path.isdir(expected_folder):\n",
        "        subdirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
        "        print(\"Folders in output directory:\", subdirs)\n",
        "        raise RuntimeError(\"Expected output folder not found in Spleeter output.\")\n",
        "\n",
        "    vocals_path = os.path.join(expected_folder, \"vocals.wav\")\n",
        "    instrumental_path = os.path.join(expected_folder, \"accompaniment.wav\")\n",
        "    if not os.path.isfile(vocals_path):\n",
        "        print(\"Files in expected folder:\", os.listdir(expected_folder))\n",
        "        raise RuntimeError(\"Vocals file not found. Spleeter separation failed.\")\n",
        "    if not os.path.isfile(instrumental_path):\n",
        "        print(\"Files in expected folder:\", os.listdir(expected_folder))\n",
        "        raise RuntimeError(\"Instrumental file not found. Spleeter separation failed.\")\n",
        "\n",
        "    print(f\"[INFO] Separation complete.\\n  Vocals: {vocals_path}\\n  Instrumental: {instrumental_path}\")\n",
        "    return vocals_path, instrumental_path\n",
        "\n",
        "def load_autovc_model(autovc_checkpoint=\"/content/autovc.ckpt\", device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Load a pre-trained AutoVC model from a checkpoint file.\n",
        "    Adjust this function to match your AutoVC model's loading code.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(autovc_checkpoint):\n",
        "        raise FileNotFoundError(f\"AutoVC checkpoint not found: {autovc_checkpoint}\")\n",
        "    print(f\"[INFO] Loading AutoVC model from {autovc_checkpoint} ...\")\n",
        "    checkpoint = torch.load(autovc_checkpoint, map_location=device)\n",
        "    # Placeholder: adjust this to your modelâ€™s actual architecture and loading method.\n",
        "    model = checkpoint\n",
        "    print(\"[INFO] AutoVC model loaded (placeholder).\")\n",
        "    return model\n",
        "\n",
        "def extract_speaker_embedding(user_voice_path, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Extract speaker embedding from your voice sample.\n",
        "    This is a naive placeholder; for production, use a proper speaker encoder.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Extracting speaker embedding from your voice sample...\")\n",
        "    audio, sr = librosa.load(user_voice_path, sr=22050)\n",
        "    mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
        "    # Naively compute the mean across time as a \"fake\" embedding.\n",
        "    speaker_embedding = np.mean(mel, axis=1)\n",
        "    speaker_embedding_tensor = torch.tensor(speaker_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    print(\"[INFO] Speaker embedding extraction complete (placeholder).\")\n",
        "    return speaker_embedding_tensor\n",
        "\n",
        "def wav_to_mel(audio, sr=22050, n_fft=1024, hop_length=256, n_mels=80):\n",
        "    \"\"\"\n",
        "    Convert a waveform to a log-mel spectrogram.\n",
        "    \"\"\"\n",
        "    spectrogram = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
        "    magnitudes, _ = librosa.magphase(spectrogram)\n",
        "    mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    mel_spectrogram = np.dot(mel_basis, magnitudes)\n",
        "    mel_spectrogram = np.maximum(1e-5, mel_spectrogram)\n",
        "    log_mel = np.log10(mel_spectrogram)\n",
        "    return log_mel\n",
        "\n",
        "def griffin_lim(magnitudes, n_iter=60, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Griffin-Lim algorithm to invert a spectrogram to waveform.\n",
        "    \"\"\"\n",
        "    angles = np.exp(2j * np.pi * np.random.rand(*magnitudes.shape))\n",
        "    for i in range(n_iter):\n",
        "        complex_spec = magnitudes * angles\n",
        "        signal = librosa.istft(complex_spec, hop_length=hop_length)\n",
        "        reconstruction = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
        "        angles = np.exp(1j * np.angle(reconstruction))\n",
        "    return signal\n",
        "\n",
        "def mel_to_audio(mel_spectrogram, sr=22050, n_fft=1024, hop_length=256):\n",
        "    \"\"\"\n",
        "    Convert a log-mel spectrogram to audio using naive inversion and Griffin-Lim.\n",
        "    \"\"\"\n",
        "    mel_spectrogram = np.power(10.0, mel_spectrogram)  # Invert the log scale.\n",
        "    mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=mel_spectrogram.shape[0])\n",
        "    inv_mel_basis = np.linalg.pinv(mel_basis)\n",
        "    linear_magnitude = np.dot(inv_mel_basis, mel_spectrogram)\n",
        "    audio = griffin_lim(linear_magnitude, n_iter=60, n_fft=n_fft, hop_length=hop_length)\n",
        "    return audio\n",
        "\n",
        "def autovc_inference(model, source_vocals_path, speaker_embedding, device=\"cpu\", output_path=\"converted_vocals.wav\"):\n",
        "    \"\"\"\n",
        "    Run AutoVC inference on the source vocals.\n",
        "    Adjust the forward pass to match your AutoVC implementation.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Running AutoVC inference on the source vocals...\")\n",
        "    source_audio, sr = librosa.load(source_vocals_path, sr=22050)\n",
        "    source_mel = wav_to_mel(source_audio, sr=sr)\n",
        "    source_mel_tensor = torch.tensor(source_mel, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        if isinstance(model, dict) and 'convert' in model:\n",
        "            converted_mel_tensor = model['convert'](source_mel_tensor, speaker_embedding)\n",
        "        else:\n",
        "            # Fallback: simply pass through the source mel spectrogram.\n",
        "            converted_mel_tensor = source_mel_tensor.clone()\n",
        "    converted_mel = converted_mel_tensor.squeeze().cpu().numpy()\n",
        "    converted_audio = mel_to_audio(converted_mel, sr=sr)\n",
        "    sf.write(output_path, converted_audio, sr)\n",
        "    print(f\"[INFO] AutoVC inference complete. Converted vocals saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def merge_audio_files(vocals_path, instrumental_path, output_path=\"final_cover.mp3\"):\n",
        "    \"\"\"\n",
        "    Merge the converted vocals with the instrumental track using FFmpeg.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Merging converted vocals with instrumental...\")\n",
        "    cmd = (\n",
        "        f\"ffmpeg -y -i {instrumental_path} -i {vocals_path} \"\n",
        "        f\"-filter_complex '[0:a][1:a]amix=inputs=2:duration=longest' {output_path}\"\n",
        "    )\n",
        "    subprocess.call(cmd, shell=True)\n",
        "    print(f\"[INFO] Final cover saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# ----- Main Pipeline -----\n",
        "def main():\n",
        "    # Prompt for user inputs.\n",
        "    youtube_url = input(\"Enter the YouTube URL for the song: \").strip()\n",
        "    user_voice_path = input(\"Enter the path to your own voice sample (e.g., /content/my_voice.wav): \").strip()\n",
        "    autovc_ckpt_path = \"/content/autovc.ckpt\"  # Ensure this file is in your Colab environment\n",
        "\n",
        "    # Step 1: Download the song.\n",
        "    downloaded_song = download_youtube_audio(youtube_url, \"downloaded_song.mp3\")\n",
        "\n",
        "    # Step 2: Separate vocals and instrumental.\n",
        "    vocals_path, instrumental_path = separate_vocals(downloaded_song)\n",
        "\n",
        "    # Step 3a: Load the AutoVC model.\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    autovc_model = load_autovc_model(autovc_ckpt_path, device=device)\n",
        "\n",
        "    # Step 3b: Extract your speaker embedding.\n",
        "    speaker_emb = extract_speaker_embedding(user_voice_path, device=device)\n",
        "\n",
        "    # Step 3c: Run AutoVC inference on the source vocals.\n",
        "    converted_vocals_path = autovc_inference(\n",
        "        model=autovc_model,\n",
        "        source_vocals_path=vocals_path,\n",
        "        speaker_embedding=speaker_emb,\n",
        "        device=device,\n",
        "        output_path=\"converted_vocals.wav\"\n",
        "    )\n",
        "\n",
        "    # Step 4: Merge the converted vocals with the instrumental.\n",
        "    final_output = merge_audio_files(converted_vocals_path, instrumental_path, \"final_cover.mp3\")\n",
        "    print(f\"[INFO] Your AI-generated cover is ready: {final_output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO9u3ObRYfEI",
        "outputId": "759efcf8-f580-44c4-92a1-78ede063eaf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.8.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Enter the YouTube URL for the song: https://www.youtube.com/watch?v=zRtPUIumXcY\n",
            "Enter the path to your own voice sample (e.g., /content/my_voice.wav): /content/naat-wav.wav\n",
            "[INFO] Downloading YouTube audio using yt-dlp...\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zRtPUIumXcY\n",
            "[youtube] zRtPUIumXcY: Downloading webpage\n",
            "[youtube] zRtPUIumXcY: Downloading tv client config\n",
            "[youtube] zRtPUIumXcY: Downloading player f6e09c70\n",
            "[youtube] zRtPUIumXcY: Downloading tv player API JSON\n",
            "[youtube] zRtPUIumXcY: Downloading ios player API JSON\n",
            "[youtube] zRtPUIumXcY: Downloading m3u8 information\n",
            "[info] zRtPUIumXcY: Downloading 1 format(s): 251\n",
            "[download] Destination: downloaded_song.mp3\n",
            "[download] 100% of    2.72MiB in 00:00:00 at 9.73MiB/s   \n",
            "[ExtractAudio] Destination: downloaded_song.mp3.mp3\n",
            "Deleting original file downloaded_song.mp3 (pass -k to keep)\n",
            "[INFO] Download completed: downloaded_song.mp3.mp3\n",
            "[INFO] Separating vocals and instrumental with Spleeter...\n",
            "[INFO] Separation complete.\n",
            "  Vocals: separated/downloaded_song.mp3/vocals.wav\n",
            "  Instrumental: separated/downloaded_song.mp3/accompaniment.wav\n",
            "[INFO] Loading AutoVC model from /content/autovc.ckpt ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-ab75909ea7bd>:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(autovc_checkpoint, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] AutoVC model loaded (placeholder).\n",
            "[INFO] Extracting speaker embedding from your voice sample...\n",
            "[INFO] Speaker embedding extraction complete (placeholder).\n",
            "[INFO] Running AutoVC inference on the source vocals...\n",
            "[INFO] AutoVC inference complete. Converted vocals saved to: converted_vocals.wav\n",
            "[INFO] Merging converted vocals with instrumental...\n",
            "[INFO] Final cover saved to: final_cover.mp3\n",
            "[INFO] Your AI-generated cover is ready: final_cover.mp3\n"
          ]
        }
      ]
    }
  ]
}